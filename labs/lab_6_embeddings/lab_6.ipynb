{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Word2Vec from scratch in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll write Word2Vec, a particularly popular, simple, and powerful model for unsupervised learning of embedding vectors for words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Word2Vec\n",
    "Word2Vec trains embeddings by using them as input to a logistic regression model, trained to predict a word given nearby words.\n",
    "\n",
    "Alternatively, you can think of Word2Vec as a neural network with linear activations and one hidden layer, trained to predict a word given nearby words.\n",
    "In this interpretation, the input and output of the model are both one-hot encoded vectors, and the embeddings of the words are the activations of the hidden layer given that word's one-hot encoding as input.\n",
    "Since the inputs are one-hot encoded, it's more efficient (but equivalent) to use an embedding lookup instead of a matrix multiply for the first layer.\n",
    "\n",
    "The data is produced by taking any large body of text (e.g. Wikipedia) and creating (context, target) pairs, where the target is any word and the context is the $n$ words to its left and right.\n",
    "\n",
    "There are two common ways of training Word2Vec:\n",
    " - The \"skip-gram\" model uses the target word as input and predicts context words.\n",
    " - The \"continuous bag-of-words\" (CBOW) model uses the context words as input and predicts the target word\n",
    " \n",
    "We'll focus on the CBOW model, since it tends to work better for small datasets.\n",
    "\n",
    "The process of generating the data for CBOW is as follows:\n",
    " 1. Tokenize (convert words, which are strings, to integer \"tokens\" indicating the word) the dataset by assigning each word a unique integer (integer encoding)\n",
    " 2. For each word in the dataset, create a single (`context`, `target`) pair, where `target` is the integer encoding of the word and `context` is a list of the integer encodings of the $n=1$ words to the left and right of the target word\n",
    "\n",
    "(I've already written the code to do this for you below)\n",
    " \n",
    "Then, the model is trained to predict the one-hot encoding of the target word given the integer encodings of the context words:\n",
    " 1. For each context word, look up its embedding in a table\n",
    " 2. Combine these into an \"average context embedding,\" which is the depth-wise average of the embeddings of the individual context words. This results in a single embedding vector, which acts as the \"total context\" in some sense.\n",
    " 3. Perform a logistic regression (equivalently, a single dense layer with softmax activation) to predict the target word using the average context embedding as input.\n",
    "\n",
    "Instead of full logistic regression, which uses the softmax function over all of the many words that appear in the dataset, we will use candidate sampling (specifically, noise-contrastive estimation) to compute the loss.\n",
    "This should speed up training significantly.\n",
    "\n",
    "For more info on Word2Vec, see [TensorFlow's \"Vector Representations of Words\" tutorial](https://www.tensorflow.org/tutorials/representation/word2vec), or Alex Minnaar's two tutorials, [one on the skip-gram model](http://alexminnaar.com/author/word2vec-tutorial-part-i-the-skip-gram-model.html) and [the other on the CBOW model](http://alexminnaar.com/author/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Download and preprocess the data\n",
    "The dataset is the Cornell Movie-Dialogs Corpus, a collection of dialogue from movie scripts.\n",
    "Download it from the link [here](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), then unzip it in a subfolder called \"data\".\n",
    "Your directory tree should include the file `./data/cornell movie-dialogs corpus/movie_lines.txt`.\n",
    "\n",
    "I've written all the code for loading and preprocessing the data below, but read through it to understand what's going on.\n",
    "You'll need to use parts of it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1: Read the data\n",
    "First, we read the lines as separate strings from the text file.\n",
    "\n",
    "If you are running into memory problems (i.e. running out of RAM while working on this assignment) you can increase `skip_header` to train on a smaller dataset (but your resulting embeddings will be less good).\n",
    "There are about 300,000 lines total, so you can set `skip_header` to skip some percentage of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings on incorrectly-formatted inputs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "_, _, _, _, lines = np.genfromtxt(\n",
    "    './data/cornell movie-dialogs corpus/movie_lines.txt',\n",
    "    dtype='<U128', \n",
    "    delimiter='+++$+++', autostrip=True,\n",
    "    encoding='latin1', invalid_raise=False, unpack=True,\n",
    "    skip_header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2: Tokenize\n",
    "Keras has a built-in tokenization utility, which we'll use.\n",
    "This creates two dictionaries, which you'll need to use later:\n",
    " - `tokenizer.word_index` maps from string words to integer tokens\n",
    " - `tokenizer.index_word` maps from integer tokens to string words\n",
    "\n",
    "Then, we convert the dialogue lines into lists of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The integer encoding of \"yes\" is: 67\n",
      "The word with integer encoding 10 is: what\n"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer and assign each word an integer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "\n",
    "# Convert the lines to lists of integers\n",
    "tokenized_lines = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# Delete the original lines to save memory\n",
    "del(lines)\n",
    "\n",
    "# Here's an example of how to use word_index and index_word\n",
    "print('The integer encoding of \"yes\" is:',\n",
    "      tokenizer.word_index['yes'])\n",
    "print('The word with integer encoding 10 is:',\n",
    "      tokenizer.index_word[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3: Create (context, target) pairs\n",
    "In this case, \"target\" means the integer encoding of a word, and \"context\" means a list of the integer encodings of the words to its left and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_words: 54034\n",
      "n_examples: 1178349\n"
     ]
    }
   ],
   "source": [
    "window_size = 1  # Consider 1 word on each side of target\n",
    "stride = 2       # Avoid window overlap\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Add (context, target) pairs to the dataset\n",
    "for line in tokenized_lines:\n",
    "    # Do not use lines that are too short\n",
    "    if len(line) < 2 * window_size + 1:\n",
    "        continue\n",
    "    \n",
    "    for target_idx in range(window_size, \n",
    "                            len(line) - window_size, \n",
    "                            stride):\n",
    "        target = line[target_idx]\n",
    "        left_context = line[target_idx - window_size : \n",
    "                            target_idx]\n",
    "        right_context = line[target_idx + 1 :\n",
    "                             target_idx + window_size + 1]\n",
    "        \n",
    "        Y.append(target)\n",
    "        X.append(left_context + right_context)\n",
    "\n",
    "# Convert to ndarrays\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "# These constants may be useful for you later\n",
    "n_words = max(tokenizer.word_index.values())\n",
    "n_examples = len(X)\n",
    "\n",
    "print('n_words:', n_words)\n",
    "print('n_examples:', n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4: Make a TSV metadata file\n",
    "This creates a metadata file in the format that the TensorBoard Projector uses (tab-separated values).\n",
    "This will let us see the names for words later when we visualize the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make a logs directory if none exists yet\n",
    "os.makedirs('./logs', exist_ok=True)\n",
    "\n",
    "# Make TSV metadata file\n",
    "with open('./logs/metadata.tsv', 'w') as f:\n",
    "    # Header specifies column name\n",
    "    f.write('Word\\tIndex\\tCount\\n')\n",
    "    \n",
    "    # Unrecognized values have an integer encoding of 0\n",
    "    f.write('UNK\\t0\\t0\\n')\n",
    "    \n",
    "    # One word per line\n",
    "    for i in range(1, n_words):\n",
    "        word = tokenizer.index_word[i]\n",
    "        count = tokenizer.word_counts[word]\n",
    "        f.write('{}\\t{}\\t{}\\n'.format(word, i, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5: Build a TensorFlow data pipeline\n",
    "I've set up the `tf.data.Dataset` and the iterator for you.\n",
    "Feel free to change the training hyperparameters and transforms if you like.\n",
    "\n",
    "You might want to try:\n",
    " - Changing the batch size to suit your RAM / GPU memory situation\n",
    " - Removing `.cache()` from the transforms if you can't fit the whole dataset in RAM\n",
    " - Changing n_negative_samples. Increasing it makes computing the loss slower but more accurate per batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: 4602.92578125\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "n_epochs = 5\n",
    "batch_size = 256\n",
    "n_batches_per_epoch = n_examples / batch_size\n",
    "print('Batches per epoch:', n_batches_per_epoch)\n",
    "\n",
    "# Number of wrong words to sample when computing\n",
    "# the loss using noise-contrastive estimation.\n",
    "n_negative_samples = 256\n",
    "\n",
    "# Construct dataset and apply transforms\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\\\n",
    "    .shuffle(1000)\\\n",
    "    .batch(batch_size)\\\n",
    "    .cache()\\\n",
    "    .repeat(n_epochs)\n",
    "\n",
    "# A one-shot iterator should be fine here\n",
    "iterator = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Build a model graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0: Model hyperparameters\n",
    "`embedding_size` is the number of dimensions in the embedding vector of each word.\n",
    "Try to train 64-dimensional embeddings, but if training takes too long, feel free to reduce this to 32 or 16.\n",
    "\n",
    "`validation_words` is a list of words we'll use to see how good our embeddings are.\n",
    "While training, we'll periodically print out the words that have embeddings most similar to these words.\n",
    "As training progresses, this should start printing words with similar meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "validation_words = ['yes', 'small', 'thousand']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Input tensors\n",
    "Using the iterator, get a tensor which holds the context integer encodings (shape: `(batch_size, 2)`) and a tensor which holds the target integer encoding (shape: `(batch_size,)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('inputs'):\n",
    "    next_elem = iterator.get_next()\n",
    "    context = tf.identity(next_elem[0], name='features')\n",
    "    target = tf.identity(next_elem[1],name='digit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Embedding variable\n",
    "Make a variable which holds the embeddings for each of the words.\n",
    "It should be a rank-2 tensor (a matrix) where the $i$-th row is the embedding vector for the word with integer encoding $i$.\n",
    "Its shape should be `(n_words, embedding_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "embeddings = tf.get_variable('embeddings', shape=(n_words, embedding_size), dtype=tf.float32,\n",
    "                          initializer=tf.glorot_uniform_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Compute the average context embedding\n",
    "Look up the embeddings of each of the context words (with a single call to `tf.nn.embedding_lookup`), then average them depth-wise into a single average embedding vector with shape `(batch_size, embedding_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = tf.nn.embedding_lookup(embeddings, context,\n",
    "                                         name='word_embeddings')\n",
    "\n",
    "word_embeddings = tf.reduce_mean(word_embeddings,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4: Create the variables for logistic regression\n",
    "Create weight and bias variables for a logistic regression which takes in the average context embedding and outputs a probability for each word.\n",
    "\n",
    "NOTE: We won't actually ever compute the output of this logistic regression by hand (i.e. the matrix multiplication and softmax activation), since we're using noise-contrastive estimation to approximate it.\n",
    "TensorFlow's `nce_loss` function just takes the weights and bias tensors as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.get_variable('weights', shape=(n_words, embedding_size), dtype=tf.float32,\n",
    "                          initializer=tf.glorot_uniform_initializer())\n",
    "bias = tf.get_variable('bias', shape=(n_words), dtype=tf.float32,\n",
    "                       initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Compute the loss\n",
    "We want to jointly train the logistic regression weights and the word embeddings using cross-entropy loss on the output of the logistic regression, but this is inefficient because of the large number of outputs (we'd need one logit for each word).\n",
    "\n",
    "Instead, approximate the per-example loss using noise-contrastive estimation by calling `tf.nn.nce_loss`, then compute the mean loss for the batch, and add a summary scalar to plot the loss in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_per_example = tf.nn.nce_loss(weights, bias, tf.expand_dims(target,-1), word_embeddings, n_negative_samples, n_words)\n",
    "loss_batch = tf.reduce_mean(loss_per_example)\n",
    "\n",
    "tf.summary.scalar('loss', loss_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7: Optimizer and gradients\n",
    "Make an optimizer (I used `tf.train.AdamOptimizer` with `learning_rate=1e-3`) and an operation to apply the gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-1)\n",
    "optimize_step = optimizer.minimize(loss_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8: Operations find similar embeddings\n",
    "The **cosine similarity** between two embeddings $\\vec{x}$ and $\\vec{y}$ is the cosine of the angle between them, computed as:\n",
    "$$\n",
    "\\cos(\\vec{x}, \\vec{y}) = \\frac{\\vec{x} \\cdot \\vec{y}}{||\\vec{x}|| \\cdot ||\\vec{y}||}\n",
    "$$\n",
    "This is a more robust similarity measure than Euclidean distance, since it's invariant to scaling the embedding vectors by a constant.\n",
    "\n",
    "We'll use cosine similarity to find the most similar words to any given input word.\n",
    "The steps for doing this are:\n",
    " 1. Take a word's integer encoding as input, and compute its embedding (I've done this for you)\n",
    " 2. Compute a \"similarity tensor\" which holds cosine similarity of this embedding with all of the word embeddings. This should result in a single tensor with shape `(n_words,).\n",
    " 3. Use `tf.nn.top_k` to find the top 8 similarities and their indices in the similarity tensor. These indices will be the integer encodings of the words with the most similar embeddings to the input.\n",
    "\n",
    "Later, you can find the most similar words to a given input word by running (in a session) the tensor that holds the top 8 indices, then using `tokenizer.index_word` on each of those indices.\n",
    "\n",
    "When debugging this, make sure that the most similar word to a given word is that word itself, with a cosine similarity of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('compute_similar_words'):\n",
    "    input_word = tf.placeholder(tf.int64, shape=(), name='input_word')\n",
    "    input_embedding = tf.nn.embedding_lookup(embeddings,\n",
    "                                             input_word,\n",
    "                                             name='input_embedding')\n",
    "    \n",
    "    similarity = tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(input_embedding,0), tf.nn.l2_normalize(embeddings,1)), axis=1)\n",
    "    top_8_vals, top_8_indices = tf.nn.top_k(similarity, 8)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Train the model\n",
    "Use the same kind of training loop we've used before, repeatedly calling the operation that applies gradient updates.\n",
    "\n",
    "The dataset is large, but the model doesn't do much work for each example.\n",
    "So, you might want to only run and save the summaries every 1000 batches or so to speed up training.\n",
    "\n",
    "In addition, every 1000 batches, for every word in `validation_words`, print the 8 most similar words by cosine similarity.\n",
    "\n",
    "Finally, use a `tf.train.Saver()` to save the model in `./logs`, which is necessary to visualize the embeddings in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  1000\n",
      "loss: 198.89507\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  thought         closeness:  0.76607573\n",
      "word:  oh         closeness:  0.7412616\n",
      "word:  too         closeness:  0.72312057\n",
      "word:  no         closeness:  0.70818365\n",
      "word:  hope         closeness:  0.6963681\n",
      "word:  but         closeness:  0.6856843\n",
      "word:  well         closeness:  0.6856738\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  padua         closeness:  0.62224144\n",
      "word:  w         closeness:  0.54033947\n",
      "word:  again         closeness:  0.5189366\n",
      "word:  lemme         closeness:  0.5139601\n",
      "word:  penny         closeness:  0.5125702\n",
      "word:  they         closeness:  0.508471\n",
      "word:  would'd've         closeness:  0.508217\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  ladies         closeness:  0.63700217\n",
      "word:  carl         closeness:  0.6286256\n",
      "word:  twombley         closeness:  0.6218487\n",
      "word:  center         closeness:  0.6107118\n",
      "word:  possibility         closeness:  0.6093341\n",
      "word:  diet         closeness:  0.6042172\n",
      "word:  whore         closeness:  0.59461933\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  2000\n",
      "loss: 85.92302\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  0.99999994\n",
      "word:  hey         closeness:  0.7952338\n",
      "word:  fudge         closeness:  0.78379524\n",
      "word:  existence         closeness:  0.7783729\n",
      "word:  declared         closeness:  0.7751378\n",
      "word:  archbishop         closeness:  0.7749965\n",
      "word:  ev         closeness:  0.77385575\n",
      "word:  as         closeness:  0.768698\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  legendary         closeness:  0.86862993\n",
      "word:  heavy         closeness:  0.86734927\n",
      "word:  association         closeness:  0.8652381\n",
      "word:  communist         closeness:  0.86427486\n",
      "word:  golly         closeness:  0.86199594\n",
      "word:  correspondent         closeness:  0.8617795\n",
      "word:  numb         closeness:  0.8617033\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  ransom         closeness:  0.8471585\n",
      "word:  companies         closeness:  0.8356945\n",
      "word:  hash         closeness:  0.82691336\n",
      "word:  arithmetic         closeness:  0.8247021\n",
      "word:  satellite         closeness:  0.8202008\n",
      "word:  ev         closeness:  0.8172374\n",
      "word:  etc         closeness:  0.8171476\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  3000\n",
      "loss: 47.012173\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  and         closeness:  0.6923745\n",
      "word:  yeah         closeness:  0.68947995\n",
      "word:  well         closeness:  0.66832334\n",
      "word:  god         closeness:  0.66345674\n",
      "word:  now         closeness:  0.64997417\n",
      "word:  mom         closeness:  0.63697964\n",
      "word:  oh         closeness:  0.62017417\n",
      "validation word:  small\n",
      "word:  small         closeness:  0.99999994\n",
      "word:  rule         closeness:  0.65781975\n",
      "word:  advance         closeness:  0.6497402\n",
      "word:  term         closeness:  0.6402605\n",
      "word:  barks         closeness:  0.632519\n",
      "word:  proud         closeness:  0.6243642\n",
      "word:  cancer         closeness:  0.6236248\n",
      "word:  holiday         closeness:  0.6235425\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  minutes         closeness:  0.71624213\n",
      "word:  million         closeness:  0.6868234\n",
      "word:  o'clock         closeness:  0.6451131\n",
      "word:  weeks         closeness:  0.6418383\n",
      "word:  experiencing         closeness:  0.62117505\n",
      "word:  months         closeness:  0.6181129\n",
      "word:  latisha         closeness:  0.6159716\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  4000\n",
      "loss: 123.935394\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  told         closeness:  0.7855517\n",
      "word:  kind         closeness:  0.7802428\n",
      "word:  let         closeness:  0.7775805\n",
      "word:  father         closeness:  0.7712593\n",
      "word:  danger         closeness:  0.76434755\n",
      "word:  when         closeness:  0.7642995\n",
      "word:  death         closeness:  0.7579343\n",
      "validation word:  small\n",
      "word:  small         closeness:  0.9999999\n",
      "word:  ooo         closeness:  0.9716683\n",
      "word:  wearing         closeness:  0.9666958\n",
      "word:  c'est         closeness:  0.9657575\n",
      "word:  rug         closeness:  0.96456015\n",
      "word:  standing         closeness:  0.9644712\n",
      "word:  truck         closeness:  0.9641364\n",
      "word:  caitlin         closeness:  0.9619024\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  million         closeness:  0.78597534\n",
      "word:  hundred         closeness:  0.75487125\n",
      "word:  five         closeness:  0.72805494\n",
      "word:  minutes         closeness:  0.6777193\n",
      "word:  troops         closeness:  0.67383456\n",
      "word:  bucks         closeness:  0.65075684\n",
      "word:  days         closeness:  0.64276797\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  5000\n",
      "loss: 29.325924\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  0.9999999\n",
      "word:  well         closeness:  0.8128296\n",
      "word:  sorry         closeness:  0.7868098\n",
      "word:  oh         closeness:  0.7841333\n",
      "word:  jeez         closeness:  0.77262604\n",
      "word:  no         closeness:  0.77239054\n",
      "word:  sir         closeness:  0.77084935\n",
      "word:  am         closeness:  0.77061874\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0000001\n",
      "word:  aw         closeness:  0.78792274\n",
      "word:  bleeding         closeness:  0.7810546\n",
      "word:  full         closeness:  0.7793541\n",
      "word:  gentlemen         closeness:  0.77713823\n",
      "word:  cabinet         closeness:  0.77697194\n",
      "word:  fall         closeness:  0.7718736\n",
      "word:  taken         closeness:  0.7715887\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  million         closeness:  0.79352295\n",
      "word:  years         closeness:  0.7373579\n",
      "word:  five         closeness:  0.7069802\n",
      "word:  times         closeness:  0.7062995\n",
      "word:  hundred         closeness:  0.7007854\n",
      "word:  weeks         closeness:  0.6866725\n",
      "word:  percent         closeness:  0.68206024\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  6000\n",
      "loss: 105.13077\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  no         closeness:  0.8458719\n",
      "word:  well         closeness:  0.84539497\n",
      "word:  see         closeness:  0.8422115\n",
      "word:  oh         closeness:  0.8350231\n",
      "word:  then         closeness:  0.81367517\n",
      "word:  must         closeness:  0.8128021\n",
      "word:  think         closeness:  0.8008367\n",
      "validation word:  small\n",
      "word:  small         closeness:  0.9999999\n",
      "word:  big         closeness:  0.7724039\n",
      "word:  middle         closeness:  0.77067983\n",
      "word:  silly         closeness:  0.76247805\n",
      "word:  getting         closeness:  0.7594215\n",
      "word:  secret         closeness:  0.7593318\n",
      "word:  following         closeness:  0.75011104\n",
      "word:  better         closeness:  0.74643594\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  0.99999994\n",
      "word:  hundred         closeness:  0.74381495\n",
      "word:  million         closeness:  0.74281883\n",
      "word:  hours         closeness:  0.70811474\n",
      "word:  minutes         closeness:  0.69922113\n",
      "word:  horror         closeness:  0.67674625\n",
      "word:  percent         closeness:  0.6728158\n",
      "word:  dollars         closeness:  0.6704097\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  7000\n",
      "loss: 35.502266\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  what         closeness:  0.8398216\n",
      "word:  but         closeness:  0.82119966\n",
      "word:  if         closeness:  0.81573254\n",
      "word:  is         closeness:  0.8149724\n",
      "word:  i'm         closeness:  0.80406296\n",
      "word:  was         closeness:  0.8031166\n",
      "word:  something         closeness:  0.7972099\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  big         closeness:  0.668326\n",
      "word:  bad         closeness:  0.6616908\n",
      "word:  this         closeness:  0.63671374\n",
      "word:  long         closeness:  0.63301164\n",
      "word:  learning         closeness:  0.6316732\n",
      "word:  my         closeness:  0.6302757\n",
      "word:  old         closeness:  0.6232263\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  0.9999998\n",
      "word:  ruby         closeness:  0.75342417\n",
      "word:  insult         closeness:  0.7267112\n",
      "word:  intends         closeness:  0.7243616\n",
      "word:  match         closeness:  0.722082\n",
      "word:  payments         closeness:  0.7220067\n",
      "word:  shoulder         closeness:  0.7173877\n",
      "word:  teeth         closeness:  0.71643794\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  8000\n",
      "loss: 59.273624\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  not         closeness:  0.8878711\n",
      "word:  yeah         closeness:  0.8765572\n",
      "word:  but         closeness:  0.87265253\n",
      "word:  then         closeness:  0.8724737\n",
      "word:  when         closeness:  0.8707491\n",
      "word:  all         closeness:  0.86802566\n",
      "word:  see         closeness:  0.8606856\n",
      "validation word:  small\n",
      "word:  small         closeness:  0.9999998\n",
      "word:  anyway         closeness:  0.786989\n",
      "word:  from         closeness:  0.76133555\n",
      "word:  bank         closeness:  0.75432116\n",
      "word:  bid         closeness:  0.75142336\n",
      "word:  behind         closeness:  0.749396\n",
      "word:  transporter         closeness:  0.74925315\n",
      "word:  hearing         closeness:  0.7491996\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  million         closeness:  0.66269255\n",
      "word:  hundred         closeness:  0.61344934\n",
      "word:  solid         closeness:  0.5931825\n",
      "word:  came         closeness:  0.5766499\n",
      "word:  true         closeness:  0.5751262\n",
      "word:  waiting         closeness:  0.57395065\n",
      "word:  y         closeness:  0.5669819\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  9000\n",
      "loss: 25.169209\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  sure         closeness:  0.8961806\n",
      "word:  and         closeness:  0.89363\n",
      "word:  where         closeness:  0.8900567\n",
      "word:  what         closeness:  0.87993443\n",
      "word:  nothing         closeness:  0.87771773\n",
      "word:  was         closeness:  0.8757988\n",
      "word:  oh         closeness:  0.87003255\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  far         closeness:  0.7230499\n",
      "word:  clothes         closeness:  0.6717365\n",
      "word:  against         closeness:  0.6567288\n",
      "word:  quick         closeness:  0.65551966\n",
      "word:  murray         closeness:  0.6448612\n",
      "word:  amazing         closeness:  0.64386755\n",
      "word:  picking         closeness:  0.6424086\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  three         closeness:  0.8068572\n",
      "word:  lakefront         closeness:  0.80045307\n",
      "word:  bringing         closeness:  0.7847738\n",
      "word:  mouthful         closeness:  0.7808353\n",
      "word:  deputies         closeness:  0.77885836\n",
      "word:  inconvenience         closeness:  0.7721871\n",
      "word:  where         closeness:  0.77048886\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  10000\n",
      "loss: 21.594604\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  0.9999999\n",
      "word:  there's         closeness:  0.89238435\n",
      "word:  oh         closeness:  0.86754954\n",
      "word:  think         closeness:  0.8673928\n",
      "word:  see         closeness:  0.86441433\n",
      "word:  not         closeness:  0.86440825\n",
      "word:  about         closeness:  0.86415017\n",
      "word:  like         closeness:  0.86332726\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  world         closeness:  0.7471935\n",
      "word:  day         closeness:  0.7419373\n",
      "word:  through         closeness:  0.73642826\n",
      "word:  such         closeness:  0.7306794\n",
      "word:  at         closeness:  0.7260124\n",
      "word:  without         closeness:  0.725685\n",
      "word:  coming         closeness:  0.7247138\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  0.99999994\n",
      "word:  these         closeness:  0.7495667\n",
      "word:  did         closeness:  0.73165804\n",
      "word:  am         closeness:  0.72462434\n",
      "word:  maybe         closeness:  0.7214463\n",
      "word:  don't         closeness:  0.71979344\n",
      "word:  take         closeness:  0.7185625\n",
      "word:  guys         closeness:  0.71626496\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  11000\n",
      "loss: 35.0799\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  not         closeness:  0.8280104\n",
      "word:  you         closeness:  0.8143238\n",
      "word:  when         closeness:  0.8125749\n",
      "word:  do         closeness:  0.81126505\n",
      "word:  if         closeness:  0.80820495\n",
      "word:  did         closeness:  0.8014786\n",
      "word:  have         closeness:  0.79694235\n",
      "validation word:  small\n",
      "word:  small         closeness:  0.99999994\n",
      "word:  huh         closeness:  0.6773133\n",
      "word:  itÂ’s         closeness:  0.66331196\n",
      "word:  now         closeness:  0.6556122\n",
      "word:  hear         closeness:  0.65067357\n",
      "word:  are         closeness:  0.64781815\n",
      "word:  going         closeness:  0.6453218\n",
      "word:  for         closeness:  0.6423322\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  years         closeness:  0.7887764\n",
      "word:  days         closeness:  0.77509034\n",
      "word:  million         closeness:  0.76686794\n",
      "word:  hundred         closeness:  0.7576504\n",
      "word:  miles         closeness:  0.749064\n",
      "word:  fifty         closeness:  0.7468822\n",
      "word:  going         closeness:  0.74509346\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  12000\n",
      "loss: 23.162823\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  what         closeness:  0.81438345\n",
      "word:  now         closeness:  0.8001111\n",
      "word:  not         closeness:  0.7916932\n",
      "word:  so         closeness:  0.79095554\n",
      "word:  love         closeness:  0.78759366\n",
      "word:  is         closeness:  0.78532976\n",
      "word:  said         closeness:  0.7767229\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  of'em         closeness:  0.6309952\n",
      "word:  suicide's         closeness:  0.62614346\n",
      "word:  mayol         closeness:  0.6210925\n",
      "word:  katrina         closeness:  0.62087166\n",
      "word:  suicide         closeness:  0.6195191\n",
      "word:  cripple         closeness:  0.6157001\n",
      "word:  bluestar         closeness:  0.6146821\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  fifty         closeness:  0.7020585\n",
      "word:  twelve         closeness:  0.66752625\n",
      "word:  ninety         closeness:  0.65259445\n",
      "word:  two         closeness:  0.6481855\n",
      "word:  dollar         closeness:  0.64623284\n",
      "word:  words         closeness:  0.6456733\n",
      "word:  kilos         closeness:  0.644838\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  13000\n",
      "loss: 22.58276\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  no         closeness:  0.8345392\n",
      "word:  oh         closeness:  0.82289565\n",
      "word:  to         closeness:  0.8164067\n",
      "word:  right         closeness:  0.80768037\n",
      "word:  was         closeness:  0.7989187\n",
      "word:  don't         closeness:  0.7965606\n",
      "word:  funny         closeness:  0.79624844\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  hard         closeness:  0.70155704\n",
      "word:  takin'         closeness:  0.6696293\n",
      "word:  nice         closeness:  0.66677636\n",
      "word:  wit'         closeness:  0.66261375\n",
      "word:  long         closeness:  0.6625422\n",
      "word:  enjoying         closeness:  0.65771097\n",
      "word:  turn         closeness:  0.6561959\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  days         closeness:  0.6894964\n",
      "word:  lower         closeness:  0.6709317\n",
      "word:  accounts         closeness:  0.6705913\n",
      "word:  insults         closeness:  0.66914403\n",
      "word:  gotta         closeness:  0.65548575\n",
      "word:  hundred         closeness:  0.64960307\n",
      "word:  questions         closeness:  0.64883643\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  14000\n",
      "loss: 28.964705\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0000001\n",
      "word:  sure         closeness:  0.78468966\n",
      "word:  if         closeness:  0.77876997\n",
      "word:  do         closeness:  0.77186215\n",
      "word:  now         closeness:  0.7714278\n",
      "word:  have         closeness:  0.76901096\n",
      "word:  i'm         closeness:  0.763057\n",
      "word:  think         closeness:  0.75837183\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  feel         closeness:  0.70789564\n",
      "word:  ready         closeness:  0.70598406\n",
      "word:  connection         closeness:  0.69983333\n",
      "word:  fighting         closeness:  0.6978798\n",
      "word:  here         closeness:  0.691722\n",
      "word:  money         closeness:  0.68911886\n",
      "word:  in         closeness:  0.68317294\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  0.9999999\n",
      "word:  chapels         closeness:  0.6741113\n",
      "word:  choices         closeness:  0.6727211\n",
      "word:  billion         closeness:  0.6634933\n",
      "word:  days         closeness:  0.6632484\n",
      "word:  families         closeness:  0.6551472\n",
      "word:  honor         closeness:  0.65370995\n",
      "word:  carlo         closeness:  0.65142894\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  15000\n",
      "loss: 24.193466\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  0.9999999\n",
      "word:  think         closeness:  0.8611295\n",
      "word:  know         closeness:  0.856808\n",
      "word:  not         closeness:  0.85652584\n",
      "word:  do         closeness:  0.85625744\n",
      "word:  all         closeness:  0.85588086\n",
      "word:  there         closeness:  0.8531234\n",
      "word:  in         closeness:  0.852228\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  kind         closeness:  0.66093004\n",
      "word:  sense         closeness:  0.6514172\n",
      "word:  meaning         closeness:  0.6370607\n",
      "word:  thursday         closeness:  0.62935686\n",
      "word:  access         closeness:  0.6254429\n",
      "word:  spare         closeness:  0.621787\n",
      "word:  worried         closeness:  0.6210264\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  0.9999999\n",
      "word:  idea         closeness:  0.77008915\n",
      "word:  hundred         closeness:  0.76633763\n",
      "word:  questions         closeness:  0.74994165\n",
      "word:  are         closeness:  0.7412946\n",
      "word:  gave         closeness:  0.73609436\n",
      "word:  frightened         closeness:  0.7320174\n",
      "word:  seven         closeness:  0.73115325\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  16000\n",
      "loss: 32.18467\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  oh         closeness:  0.74049693\n",
      "word:  need         closeness:  0.7343339\n",
      "word:  got         closeness:  0.72737634\n",
      "word:  slow         closeness:  0.72592837\n",
      "word:  think         closeness:  0.72579974\n",
      "word:  sorry         closeness:  0.72571003\n",
      "word:  well         closeness:  0.7246278\n",
      "validation word:  small\n",
      "word:  small         closeness:  0.99999994\n",
      "word:  shut         closeness:  0.7375472\n",
      "word:  others         closeness:  0.7363317\n",
      "word:  much         closeness:  0.7302731\n",
      "word:  help         closeness:  0.729054\n",
      "word:  last         closeness:  0.72863585\n",
      "word:  mother         closeness:  0.72847843\n",
      "word:  liquor         closeness:  0.72642976\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  lately         closeness:  0.7048798\n",
      "word:  long         closeness:  0.6859984\n",
      "word:  quick         closeness:  0.66313\n",
      "word:  fuck         closeness:  0.6620136\n",
      "word:  fiancee         closeness:  0.6620052\n",
      "word:  fucking         closeness:  0.6502642\n",
      "word:  points         closeness:  0.64789224\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  17000\n",
      "loss: 31.685677\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  0.9999999\n",
      "word:  and         closeness:  0.8357178\n",
      "word:  it's         closeness:  0.82538307\n",
      "word:  sure         closeness:  0.8246415\n",
      "word:  not         closeness:  0.82383174\n",
      "word:  there's         closeness:  0.8140194\n",
      "word:  something         closeness:  0.80934834\n",
      "word:  he's         closeness:  0.80713034\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  hand         closeness:  0.73530036\n",
      "word:  tight         closeness:  0.7107283\n",
      "word:  build         closeness:  0.70458156\n",
      "word:  grew         closeness:  0.703156\n",
      "word:  sealed         closeness:  0.7019359\n",
      "word:  fear         closeness:  0.7013756\n",
      "word:  hide         closeness:  0.7000269\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  0.9999999\n",
      "word:  years         closeness:  0.7770823\n",
      "word:  blocks         closeness:  0.7395137\n",
      "word:  must         closeness:  0.70930064\n",
      "word:  expected         closeness:  0.70315576\n",
      "word:  ten         closeness:  0.69594514\n",
      "word:  saavik         closeness:  0.69277406\n",
      "word:  hours         closeness:  0.6912846\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  18000\n",
      "loss: 44.797115\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  do         closeness:  0.8566751\n",
      "word:  guess         closeness:  0.8542298\n",
      "word:  thought         closeness:  0.84633696\n",
      "word:  like         closeness:  0.8460951\n",
      "word:  think         closeness:  0.83550966\n",
      "word:  what's         closeness:  0.8309865\n",
      "word:  have         closeness:  0.83083737\n",
      "validation word:  small\n",
      "word:  small         closeness:  0.9999999\n",
      "word:  sitting         closeness:  0.7417431\n",
      "word:  floating         closeness:  0.7417053\n",
      "word:  fun         closeness:  0.7149476\n",
      "word:  won't         closeness:  0.7103497\n",
      "word:  giving         closeness:  0.70843196\n",
      "word:  shield         closeness:  0.7070193\n",
      "word:  exactly         closeness:  0.70641327\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  took         closeness:  0.73804563\n",
      "word:  bucks         closeness:  0.66282487\n",
      "word:  children         closeness:  0.66166997\n",
      "word:  rounds         closeness:  0.65973914\n",
      "word:  fourteen         closeness:  0.65526557\n",
      "word:  sack         closeness:  0.64820147\n",
      "word:  went         closeness:  0.6410872\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  19000\n",
      "loss: 33.22351\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  nothing         closeness:  0.839803\n",
      "word:  oh         closeness:  0.8250549\n",
      "word:  what         closeness:  0.81939316\n",
      "word:  glad         closeness:  0.81668496\n",
      "word:  hungry         closeness:  0.8061724\n",
      "word:  afraid         closeness:  0.80420226\n",
      "word:  do         closeness:  0.80397356\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  artist         closeness:  0.62450135\n",
      "word:  actions         closeness:  0.6076195\n",
      "word:  tune         closeness:  0.5751822\n",
      "word:  sergeant         closeness:  0.5748404\n",
      "word:  obstinate         closeness:  0.57283616\n",
      "word:  retired         closeness:  0.5716117\n",
      "word:  creepy         closeness:  0.57133377\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  tempting         closeness:  0.74139416\n",
      "word:  berger         closeness:  0.73738265\n",
      "word:  years         closeness:  0.71112496\n",
      "word:  months         closeness:  0.70940983\n",
      "word:  minutes         closeness:  0.7035229\n",
      "word:  fifty         closeness:  0.7004397\n",
      "word:  rounds         closeness:  0.6990268\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  20000\n",
      "loss: 30.458027\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0000002\n",
      "word:  but         closeness:  0.7919669\n",
      "word:  for         closeness:  0.77407295\n",
      "word:  is         closeness:  0.77211285\n",
      "word:  here         closeness:  0.7677735\n",
      "word:  appreciate         closeness:  0.75966465\n",
      "word:  saying         closeness:  0.7561358\n",
      "word:  just         closeness:  0.75504136\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  thelma         closeness:  0.7050506\n",
      "word:  destroyed         closeness:  0.69327533\n",
      "word:  necessary         closeness:  0.6895058\n",
      "word:  spanish         closeness:  0.68921226\n",
      "word:  talking         closeness:  0.68285334\n",
      "word:  filming         closeness:  0.6770321\n",
      "word:  busted         closeness:  0.6757457\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  years         closeness:  0.7443782\n",
      "word:  seven         closeness:  0.6888509\n",
      "word:  billion         closeness:  0.6867697\n",
      "word:  yards         closeness:  0.68255657\n",
      "word:  '84         closeness:  0.68105876\n",
      "word:  five         closeness:  0.6754209\n",
      "word:  dollars         closeness:  0.6703608\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  21000\n",
      "loss: 22.6668\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0000001\n",
      "word:  sayin'         closeness:  0.78662837\n",
      "word:  sick         closeness:  0.77073956\n",
      "word:  play         closeness:  0.76758635\n",
      "word:  split         closeness:  0.7663276\n",
      "word:  much         closeness:  0.7578557\n",
      "word:  see         closeness:  0.75029564\n",
      "word:  carry         closeness:  0.7497673\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0\n",
      "word:  nick         closeness:  0.697474\n",
      "word:  butt's         closeness:  0.697206\n",
      "word:  batboy         closeness:  0.68792975\n",
      "word:  walking         closeness:  0.68334246\n",
      "word:  diary         closeness:  0.6817911\n",
      "word:  courses         closeness:  0.6784604\n",
      "word:  convenient         closeness:  0.6768614\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  0.99999994\n",
      "word:  housing         closeness:  0.71070063\n",
      "word:  million         closeness:  0.69359595\n",
      "word:  cello         closeness:  0.6815047\n",
      "word:  billion         closeness:  0.6722689\n",
      "word:  pudding         closeness:  0.6721536\n",
      "word:  sixty         closeness:  0.6710931\n",
      "word:  percent         closeness:  0.6650641\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  22000\n",
      "loss: 28.39341\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  0.9999998\n",
      "word:  tough         closeness:  0.73089623\n",
      "word:  it's         closeness:  0.7274273\n",
      "word:  okay         closeness:  0.7177365\n",
      "word:  more         closeness:  0.71388006\n",
      "word:  like         closeness:  0.7122793\n",
      "word:  dealing         closeness:  0.7094712\n",
      "word:  right         closeness:  0.7092101\n",
      "validation word:  small\n",
      "word:  small         closeness:  1.0000001\n",
      "word:  whistler's         closeness:  0.6868775\n",
      "word:  sheriff's         closeness:  0.67901725\n",
      "word:  course         closeness:  0.67753\n",
      "word:  whacked         closeness:  0.6766215\n",
      "word:  all         closeness:  0.66931593\n",
      "word:  thing         closeness:  0.6692964\n",
      "word:  script         closeness:  0.6661201\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0000001\n",
      "word:  francs         closeness:  0.71031296\n",
      "word:  five         closeness:  0.6983427\n",
      "word:  o'clock         closeness:  0.69399464\n",
      "word:  eleven         closeness:  0.6609703\n",
      "word:  coupla         closeness:  0.66012734\n",
      "word:  prospects         closeness:  0.65739214\n",
      "word:  negotiate         closeness:  0.6556846\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n",
      "batch:  23000\n",
      "loss: 34.47366\n",
      "validation word:  yes\n",
      "word:  yes         closeness:  1.0\n",
      "word:  it'll         closeness:  0.7107283\n",
      "word:  did         closeness:  0.70539784\n",
      "word:  choices         closeness:  0.7051389\n",
      "word:  settle         closeness:  0.7025088\n",
      "word:  changing         closeness:  0.70240307\n",
      "word:  thirty         closeness:  0.7010405\n",
      "word:  explain         closeness:  0.70062613\n",
      "validation word:  small\n",
      "word:  small         closeness:  0.9999999\n",
      "word:  ago         closeness:  0.71597064\n",
      "word:  zach         closeness:  0.70820254\n",
      "word:  fired         closeness:  0.7079225\n",
      "word:  acknowledged         closeness:  0.705356\n",
      "word:  hazardous         closeness:  0.70471287\n",
      "word:  weeks         closeness:  0.7035304\n",
      "word:  icebox         closeness:  0.70304596\n",
      "validation word:  thousand\n",
      "word:  thousand         closeness:  1.0\n",
      "word:  hundred         closeness:  0.69112206\n",
      "word:  dollars         closeness:  0.6848854\n",
      "word:  o'clock         closeness:  0.6808839\n",
      "word:  yards         closeness:  0.66847503\n",
      "word:  bruised         closeness:  0.66774833\n",
      "word:  seven         closeness:  0.666739\n",
      "word:  noticing         closeness:  0.66636467\n",
      "[-6.8539102e-04  6.7539178e-03  4.8980191e-03 -6.6780662e-03\n",
      " -5.3944672e-03 -9.1646146e-04 -8.5066687e-03 -6.0338527e-05\n",
      "  1.0294374e-04 -6.9434941e-04  8.7148808e-03  8.3930325e-03\n",
      " -9.6592233e-03  4.6774289e-03  5.2959621e-03  1.6253944e-03\n",
      " -1.6847039e-03  5.5740476e-03 -7.1887281e-03 -3.9141155e-03\n",
      "  7.3559303e-03  4.5282021e-03  4.3703485e-03 -3.2228963e-03\n",
      "  7.6430757e-03 -9.2646144e-03 -5.0795055e-03 -2.3069540e-03\n",
      "  1.8956130e-03 -2.9488597e-03 -2.1624584e-03  7.1991459e-03\n",
      " -3.7616347e-03  7.6758061e-03  5.8523510e-03  1.2237476e-03\n",
      "  4.0663872e-04 -6.3976543e-03 -1.2434879e-03  3.6316272e-03\n",
      "  7.6484829e-03  5.7229139e-03 -1.3773702e-04 -6.7694774e-03\n",
      " -4.1666212e-03 -3.3471873e-03 -2.4667243e-03  5.7217963e-03\n",
      "  5.1572286e-03  1.4166813e-03  6.6778399e-03 -8.4263487e-03\n",
      "  1.5365575e-03  7.9675987e-03  7.9184715e-03 -5.1222001e-03\n",
      " -6.9134161e-03  4.1463915e-03  4.9884161e-03  1.1331225e-03\n",
      " -5.8645629e-03  9.2844386e-04 -9.5920302e-03  6.3855052e-03]\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "train_batch = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Make one writer for training logs and another for test logs \n",
    "    train_writer = tf.summary.FileWriter('./logs/train', graph=sess.graph)\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(embeddings.eval()[0])  \n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            if train_batch > 0: # Typical batch\n",
    "                if train_batch % 1000 == 0:\n",
    "                    summary, loss, _ = sess.run([merged_summaries, loss_batch, optimize_step])\n",
    "                    train_writer.add_summary(summary, train_batch)\n",
    "                    print('batch: ', train_batch)\n",
    "                    print('loss:', loss)\n",
    "                    for word in validation_words:\n",
    "                        print('validation word: ', word)\n",
    "                        id_num = tokenizer.word_index[word]\n",
    "                        indices, vals = sess.run([top_8_indices, top_8_vals], feed_dict={input_word:id_num})\n",
    "                        results = zip(indices, vals)\n",
    "                        for i, v in results:\n",
    "                            print('word: ', tokenizer.index_word[i], '        closeness: ', v)\n",
    "                    print(embeddings.eval()[0])        \n",
    "\n",
    "                # Run summary ops and optimization\n",
    "                else:\n",
    "                    sess.run(optimize_step)\n",
    "                train_batch += 1\n",
    "            else:\n",
    "                # On the first batch, run a full trace\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "\n",
    "                summary, loss, _ = sess.run([merged_summaries, loss_batch, optimize_step],\n",
    "                                      options=run_options,\n",
    "                                      run_metadata=run_metadata)\n",
    "\n",
    "                train_writer.add_summary(summary, train_batch)    \n",
    "                train_writer.add_run_metadata(run_metadata, \n",
    "                                              'first training batch')\n",
    "                train_batch += 1\n",
    "        except tf.errors.OutOfRangeError: # No more data\n",
    "            break\n",
    "        \n",
    "    # Save model\n",
    "    saver.save(sess, './logs/model.ckpt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./logs/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with open('./logs/embeddings.tsv', 'w') as f:\n",
    "        saver.restore(sess, './logs/model.ckpt')\n",
    "        a = embeddings.eval()\n",
    "    \n",
    "    # One word per line\n",
    "        for i in range(n_words):\n",
    "            for j in range(embedding_size):\n",
    "                if j != embedding_size - 1:\n",
    "                    f.write('{}\\t'.format(a[i][j]))\n",
    "                else:\n",
    "                    f.write('{}'.format(a[i][j]))\n",
    "            f.write('\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Visualize the learned embeddings\n",
    "Run TensorBoard pointed at `./logs` and look in the Projector tab, then use the \"Load data\" button and select the `./logs/metadata.tsv` file we created earlier to load word labels.\n",
    "\n",
    "Try typing some words into the search bar and see which words come up as most similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 (optional): Generate analogies\n",
    "Try using embedding vector arithmetic to generate analogies.\n",
    "To do this:\n",
    " - Load the saved model into a `tf.Session()` with the saver\n",
    " - Compute the embedding vector for several vectors using `feed_dict` and `tf.nn.embedding_lookup`\n",
    " - Do vector arithmetic on the computed embeddings\n",
    " - Find the most similar word embeddings by cosine similarity, then find the words that map to those embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
