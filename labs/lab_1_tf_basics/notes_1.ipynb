{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a library (usually used with Python) developed by Google Brain for training and running statistical machine learning models.\n",
    "It is extremely flexible, making almost no assumptions about your model except that you can represent it as a graph of math operations.\n",
    "As a result, it sees lots of use in developing new machine learning models, as well as efficiently training and running existing models.\n",
    "\n",
    "Advantages of TensorFlow over other frameworks:\n",
    " - Most flexible by far -- can use basically any differentiable equation as a model\n",
    " - Most popular and heavily-developed framework by far, used by numerous companies\n",
    " - Works well with CPUs, GPUs, and TPUs (Tensor Processsing Units), and across lots of devices, including distributed training\n",
    " - Incredible debugging and visualization utilities (TensorBoard and tfdbg)\n",
    " - Graph compilation can optimize your code for you\n",
    " - Gives you lots of control over hardware (e.g. whether variables live in CPU or GPU memory)\n",
    " - Open-source!\n",
    "\n",
    "Disadvantages:\n",
    " - Graph programming unfamiliar to many people\n",
    " - Hard to search documentation, lots of deprecated and soon-to-be-deprecated APIs (with TensorFlow 2.0, this will hopefully change)\n",
    " - Generally more verbose and difficult to develop models (but we will also learn Keras, a library meant to make building common models in TensorFlow very simple)\n",
    " - Can be slower than other frameworks for certain models (Apache MXNET)\n",
    " - TensorFlow itself doesn't support every kind of statistical model, most notably \"graphical models\" associated with Bayesian statistics (although, see [bayesflow](https://www.tensorflow.org/api_docs/python/tf/contrib/bayesflow)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why care?\n",
    "\n",
    "Because it lets you do all of the awesome things machine learning can do:\n",
    " - [Translate between any two human languages](https://code.fb.com/ai-research/laser-multilingual-sentence-embeddings/)\n",
    " - [Play games with superhuman skill](https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/)\n",
    " - [Make a picture look like it was painted by Picasso](http://genekogan.com/works/style-transfer/)\n",
    " - [Generate realistic human faces](https://blog.openai.com/glow/) \n",
    " - [See through walls with WiFi](https://news.mit.edu/2018/artificial-intelligence-senses-people-through-walls-0612)\n",
    " - [Have your phone make calls for you](https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html)\n",
    " - [Find every tree in the world (shameless self-plug)](https://medium.com/descarteslabs-team/descartes-labs-urban-trees-tree-canopy-mapping-3b6c85c5c9cc)\n",
    " * ...\n",
    " \n",
    " For example, the people in these pictures never existed but were synthesized by a neural network:\n",
    "![Faces generated by neural network](https://research.nvidia.com/sites/default/files/publications/representative_image_512x256.png)\n",
    "(Image credit: [Nvidia Research, Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://research.nvidia.com/publication/2017-10_Progressive-Growing-of))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of TensorFlow API levels\n",
    "![tensorflow api levels](https://3.bp.blogspot.com/-l2UT45WGdyw/Wbe7au1nfwI/AAAAAAAAD1I/GeQcQUUWezIiaFFRCiMILlX2EYdG49C0wCLcBGAs/s1600/image6.png)\n",
    "\n",
    "We'll be focusing on three of these:\n",
    " - Python frontend (or \"operations-level TensorFlow\") lets you build a computational graph by hand for maximum power and flexibility\n",
    " - Keras to build neural networks quickly and easily\n",
    " - Datasets API to load and preprocess data efficiently \n",
    " \n",
    "This covers the two most common tasks: using Keras to build a simple neural network using common kinds of layers (like fully-connected, convolutional, and recurrent layers), and using operations-level TensorFlow to develop a novel model.\n",
    " \n",
    "(Image credit: [Google Developers Blog, Introduction to TensorFlow Datasets and Estimators](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors and tensor values\n",
    "To do machine learning, you _really_ need to understand tensors and the terminology surrounding them.\n",
    "You can (for machine learning, at least) think of tensors as multidimensional arrays.\n",
    "\n",
    "![tensors](./images/tensors_diagram.jpg)\n",
    "\n",
    "In TensorFlow, `tf.Tensor` objects act as \"placeholders\" (more on this later) for _tensor values_, which are numpy arrays.\n",
    "I'll try to stick to the notation **\"tensor\"** to mean `tf.Tensor` objects and **\"tensor value\"** to mean a multidimensional numpy array.\n",
    "The terminology around tensors and tensor values is the same.\n",
    "\n",
    "A tensor consists of a number of **scalars** organized in some \"rectangular\" way.\n",
    "Scalars have a [**data type**](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType), usually a real floating-point number (`tf.float32`), integer (`tf.int32`), or boolean (`tf.bool`).\n",
    "To access a particular scalar in a tensor, provide an index in each of its **axes** (also called **dimensions**).\n",
    "The total number of axes a tensor has is its **rank**, and each axis has a fixed **size** which determines how many sub-tensors (of rank one less than the parent) you can get by indexing into just that axis.\n",
    "The total number of scalars a tensor can hold is the product of the sizes of its axes (unless it's rank-zero, then it holds a single value).\n",
    "\n",
    "All of the \"size\" properties of a tensor are encapsulated by its **shape**, which fully specifies how many axes it has, what order they come in, and what their sizes are.\n",
    "A tensor's shape is given by a tuple.\n",
    "For example:\n",
    " - a tensor with shape (3) is a vector (rank-1 tensor) with 3 elements\n",
    " - a tensor with shape (3, 4) is a matrix (rank-2 tensor) with 3 rows and 4 columns\n",
    " - a tensor with shape () is a scalar (rank-0 tensor; you need no indices to uniquely identify its only element)\n",
    " - a tensor with shape (2, 2, 2) (rank-3 tensor) is a 2x2x2 \"data cube\"\n",
    " - a tensor with shape (3, 1) is a matrix (rank-2 tensor) with 3 rows and 1 column; subtle difference from having shape (3)\n",
    " \n",
    "When you provide an index for $n$ axes of a rank-$r$ tensor, you get back a tensor with rank $r - n$, and a shape the same as the original but with the indexed axes removed.\n",
    "Numpy and TensorFlow have roughly the same notation for indexing into tensors to obtain sub-tensors.\n",
    "Similarly, some operations called **reductions** operate across one or more axes of a tensor, collapsing those axes into a scalar summarizing their elements.\n",
    "For instance, `tf.reduce_sum` applied to every axis of a tensor outputs a scalar equal to the sum of its scalars.\n",
    "`tf.reduce_mean` applied to axis 0 of a rank-2 tensor returns a vector that, for each column, contains a scalar of the mean of all of the rows in that column. \n",
    "\n",
    "Matrix transposition (which swaps the rows and columns of a matrix) has a natural extension to tensors: it just reorders their axes.\n",
    "You can \"reshape\" a tensor into a _compatible_ shape, which is to say one where the total number of scalars remains the same.\n",
    "\n",
    "I highly recommend understanding numpy indexing, including some of the advanced stuff you can do (like broadcasting, masking, and `np.where`) since a lot of this transfers to TensorFlow.\n",
    "\n",
    "For a good guide to numpy indexing, read https://realpython.com/numpy-array-programming/.\n",
    "For more on tensors, you can read https://www.tensorflow.org/guide/tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor values:\n",
      "3 \n",
      "\n",
      "[1 2 3] \n",
      "\n",
      "[[1 2]\n",
      " [3 4]] \n",
      "\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]] \n",
      "\n",
      "[[1 2 3]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examples of creating tensor values in numpy\n",
    "print('Tensor values:')\n",
    "print(np.array(3), '\\n')           # A rank-zero tensor with shape ()\n",
    "print(np.array([1, 2, 3]), '\\n')   # A rank-one tensor with shape (3)\n",
    "print(np.array([[1, 2],\n",
    "                 [3, 4]]), '\\n')   # A rank-two tensor with shape (2, 2)\n",
    "print(np.array([[[1, 2],\n",
    "                 [3, 4]],\n",
    "                [[5, 6],\n",
    "                 [7, 8]]]), '\\n')  # A rank-three tensor with shape (2, 2, 2)\n",
    "print(np.array([[1, 2, 3]]), '\\n') # A rank-two tensor with shape (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shapes:\n",
      "(2, 2)\n",
      "() \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of printing tensor shapes\n",
    "print('Tensor shapes:')\n",
    "print(np.array([[1, 2], [3, 4]]).shape)\n",
    "print(np.array(3).shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor rank: 2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of printing tensor rank\n",
    "print('Tensor rank:', \n",
    "      np.array([[1, 2], [3, 4]]).ndim,\n",
    "      '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor indexing:\n",
      "a: [[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]] \n",
      "\n",
      "a[0, 0, 0] = 1\n",
      "a[0, 0, 1] = 2\n",
      "a[0, 1, 0] = 3\n",
      "a[1, 0, 0] = 5 \n",
      "\n",
      "a[0, 0, :] = [1 2]\n",
      "a[:, 0, :] =\n",
      " [[1 2]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Examples of indexing\n",
    "print('Tensor indexing:')\n",
    "a = np.array([[[1, 2],\n",
    "                 [3, 4]],\n",
    "                [[5, 6],\n",
    "                 [7, 8]]])\n",
    "\n",
    "print('a:', a, '\\n')\n",
    "\n",
    "print('a[0, 0, 0] =', a[0, 0, 0])  # Selecting a particular scalar\n",
    "print('a[0, 0, 1] =', a[0, 0, 1])\n",
    "print('a[0, 1, 0] =', a[0, 1, 0])\n",
    "print('a[1, 0, 0] =', a[1, 0, 0], '\\n')\n",
    "\n",
    "print('a[0, 0, :] =', a[0, 0, :])   # Take the first value on axes 0 and 1, \n",
    "                                    # leaving axis 2 alone\n",
    "print('a[:, 0, :] =\\n', a[:, 0, :]) # Take the first value on axis 2, \n",
    "                                    # leaving axes 0 and 2 alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable programming\n",
    "Differentiable programming is a paradigm in which a program is represented as a **dataflow graph**: _mathematical computations organized as a directed graph_:\n",
    "![a computational graph](https://colah.github.io/posts/2015-08-Backprop/img/tree-def.png)\n",
    "\n",
    "(Image credit: [Chris Olah's blog, Calculus on Computational Graphs: Backpropagation](https://colah.github.io/posts/2015-08-Backprop/))\n",
    "\n",
    "Nodes in the graph are operations, and edges are tensors, which flow between operations (hence the name).\n",
    "When you want to evaluate an operation (say, to get its output tensor), all of the operations that produce tensors it depends on (found by stepping back one layer in the graph) are evaluated, and so on recursively.\n",
    "In this way, you can define a bunch of interconnected computations as a single graph, and when you want to compute a value, TensorFlow will run _only the computations it needs to to compute your value_.\n",
    "During a single evaluation, tensors have fixed values, so TensorFlow will cache that value in order to prevent computing the same thing multiple times.\n",
    "For instance, when evaluating \"e\" in the above graph, the value of \"b\" is computed only once despite being used twice.\n",
    "\n",
    "It's very useful to keep the graph formalism in mind when writing TensorFlow code, and we'll return to the core ideas of differentiable programming again and again in this course.\n",
    "Because you completely define the graph before running it, TensorFlow can also [optimize the graph with the XLA compiler](https://www.tensorflow.org/xla/), doing things like fusing operations and doing clever memory-saving hacks.\n",
    "\n",
    "The key feature that separates differentiable programming from ordinary graph programming is that in differentiable programming, _you only use differentiable operations_.\n",
    "What this means is that you can use the chain rule (see the next lecture, on the Backpropagation algorithm) to compute _the gradient of any value in the graph with respect to any other value in the graph_.\n",
    "This is incredibly powerful!\n",
    "\n",
    "Critically, you can use _gradient-based optimization methods_ to maximize or minimize values in the graph by changing variable tensors earlier in the graph.\n",
    "This is how we train machine learning models using TensorFlow.\n",
    "The most common family of such optimization algorithms is... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "Gradient descent is a simple and powerful algorithm for minimizing differentiable functions.\n",
    "I'll describe it only briefly here because there are [plenty](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent) [of](https://work.caltech.edu/library/101.html) [good](https://hackernoon.com/gradient-descent-aynk-7cbe95a778da) [explanations](https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0) [online](https://www.youtube.com/watch?v=jc2IthslyzM).\n",
    "\n",
    "For a differentiable function $f(\\vec{x})$, the gradient $\\nabla_\\vec{x} f$ gives the direction of steepest increase of $f$ -- the direction to move the parameters $\\vec{x}$ that increases $f$ the most, in a small region around the value of $\\vec{x}$. \n",
    "If we want to minimize $f$, we can therefore keep track of a \"current value of $\\vec{x}$\" and repeatedly take small steps in the opposite direction of the gradient.\n",
    "\n",
    "The size of step is the **learning rate**, denoted $\\alpha$.\n",
    "It should be small, or else we'll overshoot the area where we can assume the gradient really gives the direction of steepest increase (where the linear term in the Taylor series of the function no longer dominates).\n",
    "For steep gradients, we take bigger steps because we have a stronger signal of where the parameters $\\vec{x}$ should go.\n",
    "For $\\alpha$ too small, we converge too slowly to a minimum (by taking very many small steps).\n",
    "For $\\alpha$ too large, we may not be able to minimize the function at all.\n",
    "\n",
    "![gradient descent visualization](./images/gradient_descent.jpg)\n",
    "\n",
    "In machine learning, we define a value that quantifies how badly our model fits the data (the \"loss\" or \"training error\"), then use gradient descent to change the parameters of our model (made possible by differentiable programming) to minimize that value.\n",
    "\n",
    "Below is some example code that computes the gradients and applies the updates by hand to minimize $f(x) = x^2$.\n",
    "TensorFlow will do this for you (and one of the problems on the first lab is to minimize a simple function with TensorFlow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value of x: 0.00026561398887587435\n",
      "Final value of f(x): 7.05507910865531e-08\n"
     ]
    }
   ],
   "source": [
    "# An example of minimizing the function f(x) = x^2 with gradient descent\n",
    "def f(x): return x**2\n",
    "def grad_f(x): return 2*x # Gradient of f(x), computed by hand\n",
    "\n",
    "x = 10\n",
    "learning_rate = 0.05\n",
    "x_values = [x]\n",
    "y_values = [f(x)]\n",
    "\n",
    "for step in range(100):\n",
    "    x = x - learning_rate * grad_f(x)\n",
    "    x_values.append(x)\n",
    "    y_values.append(f(x))\n",
    "    \n",
    "plt.figure(figsize=(5, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(101), x_values, 'b-')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('x')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(101), y_values, 'r-')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()\n",
    "\n",
    "print('Final value of x:', x)\n",
    "print('Final value of f(x):', f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the learning rate\n",
    "There's no single learning rate that's guaranteed to work for every problem.\n",
    "When it's too high, the notion that the gradient represents the direction of steepest increase of the function breaks down -- the gradient is just the linear term in the function's Taylor series, and if you take a big step then the nonlinear terms dominate and you can wind up moving in the wrong direction, oscillating, or overshooting the minimum.\n",
    "When it's too low, the steps taken are too small and optimization will be very slow.\n",
    "\n",
    "So, when optimizing a differentiable program, you'll need to tune the learning rate.\n",
    "A good approach for this is:\n",
    " 1. Start out with a learning rate like 10^-3\n",
    " 2. If the function to be minimized is not decreasing over time (looking at plots, print statements, etc), decrease the learning rate. If it is, try increasing it. Use exponential changes here (i.e. multiply or divide by 10).\n",
    " 3. Repeat until you find the largest learning rate that the problem still converges for, and hone in by making smaller changes if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside on local minima \n",
    "Gradient descent is only guaranteed to converge to the global minimum of a _convex_ function.\n",
    "This seems like a problem, since most loss functions we want to minimize are not convex.\n",
    "However, local minima are very rare in high-dimensional space: for a function to be at a local minimum in $n$-dimensional space, it needs to curve upwards on _every_ axis.\n",
    "A $n$-dimensional \"random\" function, at a point, could curve up or down on a given axis each with probability $\\frac{1}{2}$ so a given point has probability $\\propto 2^{-n}$ of being a local minimum.\n",
    "For neural networks, the vector space of possible parameter values can have millions of dimensions.\n",
    "\n",
    "Instead, we usually care about _saddle points_, where the loss function looks locally flat.\n",
    "Modern variants of gradient descent like momentum, adagrad, and Adam approach this problem by using more information than just the gradient to make parameter updates.\n",
    "But, how to handle these points best when training neural networks (and even how much of a problem they are) is still somewhat of an open problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph programming in TensorFlow\n",
    "\n",
    "Python programs that use Tensorflow are divided into two sections: \n",
    " 1. Building a computational graph by adding tensors and operations to a graph\n",
    " 2. Using a `tf.Session` to evaluate tensors or operations by executing subgraphs\n",
    "\n",
    "## The graph\n",
    "Your code is written in Python, but most of your program logic will be in a computational graph.\n",
    "TensorFlow represents this with a `tf.Graph` object.\n",
    "TensorFlow always has a default `Graph`, and while it does support programs with multiple graphs, this usecase is rare and bug-prone.\n",
    "As a result, you'll basically never create `Graph`s by hand -- they'll just store your program in the background.\n",
    "If you do need to access the `Graph` object (for instance, to pass to the `add_graph()` method of a `FileWriter`), use `tf.get_default_graph()`.\n",
    "\n",
    "## Tensors and operations\n",
    "To build up a computational graph, you need to add tensors (directed edges) and operations (nodes) to the `Graph`.\n",
    "Operations are represented by `tf.Operation` objects, and tensors are represented by `tf.Tensor` objects.\n",
    "Most functions in TensorFlow are actually `tf.Operation` constructors, which:\n",
    " - Take zero or more tensors, and possibly some parameters, as input\n",
    " - Permanently add a new operation to the `Graph`\n",
    " - Add a directed edge from each operation that created an input tensor to the new operation\n",
    " - Return a new `tf.Tensor` object (some operations do not return a tensor)\n",
    "\n",
    "For example (quoting the [official docs on `tf.Operation`](https://www.tensorflow.org/api_docs/python/tf/Operation)), `c = tf.matmul(a, b)` creates an `Operation` of type \"MatMul\" that takes tensors `a` and `b` as input, and produces `c` as output.\n",
    "Passing `c` to another operation \"wires\" the two together.\n",
    "\n",
    "Always keep in mind that `tf.Tensor` objects are not tensor values, they're _just Python objects you use to build a graph!_\n",
    "You can only do two things with them: pass them to operations to wire operations together in the graph, or evaluate them with the `run()` method of a `tf.Session` (see below).\n",
    "Tensors do not have values until a subgraph they're in is actually evaluated.\n",
    "So printing the `tf.Tensor` object with Python's `print()` will do nothing -- use the `tf.print()` operation instead.\n",
    "\n",
    "Operations always accept a `name` keyword argument, which will give a name to the operation added to the graph and to the tensor it returns, if applicable.\n",
    "You should always name your important operations!\n",
    "It's good documentation for your code, and will make debugging with TensorBoard and tfdbg _infinitely_ easier.\n",
    "Think of operation naming like commenting: a little thing that will make debugging, maintainence, and development much easier. \n",
    "\n",
    "The simplest operation is [`tf.constant()`](https://www.tensorflow.org/api_docs/python/tf/constant), which just represents a constant value used in the graph.\n",
    "But think through how it works internally: calling `tf.constant()` actually adds a \"Constant\" operation to the graph and returns a tensor, which is still just a placeholder despite taking on the same value every time.\n",
    "Using this constant tensor means adding a directed edge from the \"Constant\" operation to wherever the constant tensor is used.\n",
    "Thinking about the internals in depth like this can prevent bugs, like the common mistake of calling an operation constructor each time you want to use its returned tensor -- this will add the same operation to the graph many times, when in fact you should store its returned tensor as a Python variable and use it to wire the operation's output to many other places.\n",
    "\n",
    "A very similar operation is [`tf.placeholder`](https://www.tensorflow.org/api_docs/python/tf/placeholder), which takes a data type and a shape to return a single tensor.\n",
    "This tensor, when evaluated as part of a subgraph, will always error unleess fed (see the next section).\n",
    "Placeholders are often used for debugging.\n",
    "\n",
    "Note: almost any time you would use a tensor, you can instead use a [\"tensor-like object\"](https://www.tensorflow.org/guide/graphs#tensor-like_objects): a numpy `ndarray`, a Python list, or a Python scalar.\n",
    "This will just act like a constant with the given value.\n",
    "\n",
    "Note 2: \"operation constructors\" aren't actually legal Python constructors, but they're wrappers to constructors for subclasses of `tf.Operation` that return different objects\n",
    "\n",
    "## Sessions and subgraph execution\n",
    "A `tf.Session` object abstracts the entire context of running code: it owns access to hardware compute and IO devices (both local and remote), contains various settings, and manages access into the TensorFlow runtime (which is language-agnostic), including automatic parallelization.\n",
    "A session is associated with only one graph, though multiple sessions can be associated with the same graph (and often you use separate sessions for training and inference, for example).\n",
    "To create a session, call the `tf.Session()` constructor, remembering to call its `close()` method when you are done to free resources.\n",
    "Or, more conveniently (and Pythonically), use it in a `with` block:\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    ...\n",
    "```\n",
    "\n",
    "Once you have a session, you can finally execute your program by evaluating parts of the graph you've built with the [`run()`](https://www.tensorflow.org/api_docs/python/tf/Session#run) method of a `tf.Session`, which takes:\n",
    " - **fetches**: one or more tensors (which will be evaluated into numpy `ndarray`s of numbers) or operations (which will be run)\n",
    " - (optional) **feed_dict**: a dictionary of \"feeds\" mapping from tensor object to tensor value, which will overwrite the value of the tensor during the run; typically used with placeholders, but can overwrite any tensor; great for debugging!\n",
    " - (optional) **options**: [advanced options modifying the run](https://www.tensorflow.org/api_docs/python/tf/RunOptions)\n",
    " - (optional) **run_metadata**: a `tf.RunMetadata()` object to store information about the run in so it can be visualized later\n",
    "`run()` then recursively evaluates the operations required to compute each tensor or run each operation, and returns the tensor values (and None for each operation).\n",
    "Fetches can be passed as any combination of single values, lists, tuples, and dictionaries; values are returned in the same shape.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_1: 3\n",
      "results_2: {'a': array(1, dtype=int32), 'b and c': [array(-1, dtype=int32), 0]}\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(3)\n",
    "b = tf.placeholder(tf.int32)\n",
    "c = a + b\n",
    "\n",
    "run_metadata = tf.RunMetadata()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    results_1 = sess.run(c, feed_dict={b: 0})\n",
    "    results_2 = sess.run({'a': a, 'b and c': [b, c]},\n",
    "                         feed_dict={a: 1, b: -1},\n",
    "                         run_metadata=run_metadata)\n",
    "\n",
    "print('results_1:', results_1)\n",
    "print('results_2:', results_2)\n",
    "\n",
    "# Note: this sometimes returns ndarrays with single elements\n",
    "# If this bothers you, you can use np.asscalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on graphs and sessions\n",
    "For an excellent introduction to the abstractions TensorFlow uses to represent its graphs and computations, you might want to read [\"TensorFlow: The Confusing Parts\"](https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/).\n",
    "To really dive deep, the author has written [another guide](https://jacobbuckman.com/post/graph-inspection/) which should help you understand how every object in the TensorFlow graph acts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "To train a model, you need parameters that persist changes across multiple runs.\n",
    "These are represented by [`tf.Variable` objects](https://www.tensorflow.org/guide/variables), which can be treated as a `tf.Tensor`.\n",
    "You can use the [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) constructor, but it's almost always moree convenient to use the [`tf.get_variable()`](https://www.tensorflow.org/api_docs/python/tf/get_variable) wrapper function, which gets an existing variable by string name or creates it if one does not exist.\n",
    "In addition to a name, a new variable needs a shape and an initializer.\n",
    "\n",
    "[Initializers](https://www.tensorflow.org/api_docs/python/tf/initializers) determine how a variable's value begins the first time it is used.\n",
    "What you plan a variable to represent will determine how you want it initialized.\n",
    "Then, before you run any of the graph code, call `sess.run(tf.global_variables_initializer())` to initialize all variables at once, or `sess.run(my_variable.initializer)` to initialize `my_variable` (less common).\n",
    "\n",
    "Note that variables persist their value _only within the context of a single session!_\n",
    "To use variables over multiple sessions, save their values to disk with a Saver (see next lecture).\n",
    "\n",
    "You can change variables by hand, but usually they're changed with optimizers that use gradient descent (see next section) to find good values automatically.\n",
    "\n",
    "The `tf.layers` API is a series of operations that contain their own variables.\n",
    "They help for certain simpler models, but are generally less flexible than managing the variables yourself.\n",
    "As a result we won't use any operations named like `tf.layers.*`.\n",
    "\n",
    "It's worth reading the [short official Variables guide](https://www.tensorflow.org/guide/variables).\n",
    "\n",
    "## Optimizers\n",
    "[Optimizers](https://www.tensorflow.org/api_docs/python/tf/train/Optimizer) automatically change variables in the graph to find values that minimze certain quantities, usually the loss.\n",
    "\n",
    "The archetypal optimizer is [`tf.train.GradientDescentOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer), and just about every other optimizer you use will be some variant of it.\n",
    "Optimizers are just Python objects, created through their constructors with some (optional or mandatory) parameters:\n",
    "```\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "```\n",
    "Then, calling the optimizer's [`minimize()`](https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#minimize) function, passing in the tensor containing the value to minimize, returns on operation.\n",
    "When this operation is run within a session, all of the variables in the same graph as the minimization operation are updated by the optimizer according to its optimization algorithm (gradient descent or a variant).\n",
    "Usually this happens many times in a \"training loop.\"\n",
    "\n",
    "```\n",
    "training_op = optimizer.minimize(loss_tensor)\n",
    "with tf.Session() as sess:\n",
    "    for training_step in range(1000):\n",
    "        sess.run(training_op)\n",
    "```\n",
    "\n",
    "Note: we haven't actually talked about backpropagation, the algorithm TensorFlow uses to efficiently compute gradients of values in the graph with respect to variables. More on this next week.\n",
    "\n",
    "## Name scopes and variable scopes\n",
    "In addition to giving single operations names, you can also use `tf.name_scope()` to give all following operations a common prefix.\n",
    "This is very helpful to group operations that form a \"block\" (and you can think of them as forming one, more complex, operation defined in terms of its inputs and outputs), and will also allow you to collapse them into a single node when visualizing the graph (next week).\n",
    "\n",
    "`tf.variable_scope()` is similar, except it affects the names of variables instead of the names of operations.\n",
    "You can also pass the keyword phrase `reuse=True` to a `tf.variable_scope()` to get a variable by the given name if it exists; otherwise TensorFlow will just throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"this_is_a_name_scope/Const:0\", shape=(), dtype=int32)\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "<tf.Variable 'variable_name:0' shape=() dtype=int32_ref>\n",
      "<tf.Variable 'this_is_a_variable_scope/variable_name:0' shape=() dtype=int32_ref>\n",
      "b is a? True\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('this_is_a_name_scope'):\n",
    "    print(tf.constant(1))\n",
    "    print(tf.get_variable('variable_name', shape=(), dtype=tf.int32))\n",
    "    \n",
    "with tf.variable_scope('this_is_a_variable_scope'):\n",
    "    a = tf.get_variable('variable_name', shape=(), dtype=tf.int32)\n",
    "    print(a)\n",
    "    \n",
    "with tf.variable_scope('this_is_a_variable_scope', reuse=True):\n",
    "    b = tf.get_variable('variable_name', shape=(), dtype=tf.int32)\n",
    "    print('b is a?', b is a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full program example\n",
    "Below is an example of performing the minimization of $f(x) = x^2$ from before with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add operations to the default graph\n",
    "with tf.name_scope('minimize_x_squared'):\n",
    "    # Create input variable\n",
    "    x = tf.get_variable('x', shape=(), dtype=tf.float32, \n",
    "                        initializer=tf.constant_initializer(10)) \n",
    "    \n",
    "    # Compute f(x) = x^2\n",
    "    # `tf.pow` takes in two tensors and returns a tensor\n",
    "    y = tf.pow(x, tf.constant(2.0), name='y')\n",
    "    \n",
    "    # Create optimization operator\n",
    "    optimize_step = tf.train.GradientDescentOptimizer(0.05, name='optimizer').minimize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value of x: 10.0\n",
      "Initial value of f(x): 100.0\n",
      "Final value of x: 0.00026561398\n",
      "Final value of f(x): 7.055079e-08\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "with tf.Session() as sess: # Create a session   \n",
    "    sess.run(tf.global_variables_initializer()) # Initialize all variables\n",
    "    \n",
    "    print('Initial value of x:', sess.run(x))\n",
    "    print('Initial value of f(x):', sess.run(y))\n",
    "    \n",
    "    for step in range(100):\n",
    "        sess.run(optimize_step)\n",
    "        \n",
    "    print('Final value of x:', sess.run(x))\n",
    "    print('Final value of f(x):', sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper graph example\n",
    "This code creates the computational graph pictured above as a TensorFlow graph, showcasing overloading on arithmetic operators and using placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('example_graph'):\n",
    "    a = tf.placeholder(tf.float32, (), name='a') # Returns a tensor that must be fed\n",
    "    b = tf.placeholder(tf.float32, (), name='b')\n",
    "    c = a + b # This is shorthand for the operator function tf.add(a, b)\n",
    "    d = b + tf.constant(1.0)\n",
    "    e = c * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 1.0\n",
      "a = 2.0\n",
      "e = 9.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('a =', sess.run(a, feed_dict={a: 1}))\n",
    "    print('a =', sess.run(a, feed_dict={a: 2}))\n",
    "    print('e =', sess.run(e, feed_dict={a: 1, b: 2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whew! That's all the basics of writing code in TensorFlow. More interesting problems and models to come :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
