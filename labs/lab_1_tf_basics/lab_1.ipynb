{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: TensorFlow Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Tensors\n",
    "If you want to review any of this material, look over https://docs.scipy.org/doc/numpy-1.15.0/user/quickstart.html and https://www.tensorflow.org/guide/tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Tensor values, shapes, rank, and axes\n",
    "Make tensor values by hand (e.g. `x = np.array([[1, 2, 3], [4, 5, 6]])`) of the following shapes:\n",
    " * a: (2, 2)\n",
    " * b: (3)\n",
    " * c: (3, 1)\n",
    " * d: (1, 3)\n",
    " * e: ()\n",
    " * f: (1)\n",
    " * g: (2, 2, 2)\n",
    " * h: (2, 3, 1, 2)\n",
    " \n",
    " For each, put its tensor rank and total number of elements in a comment.\n",
    " Yes, this is pretty boring, but it's also short and it's really important to understand what tensors of different shapes look like and how shapes, rank, and axes interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank: 2, elems: 4\n",
    "a = np.array([[1,2], [3,4]])\n",
    "\n",
    "# rank: 1, elems: 3\n",
    "b = np.array([1,2,3])\n",
    "\n",
    "# rank: 2, elems: 3\n",
    "c = np.array([[1], [2], [3]])\n",
    "\n",
    "# rank: 2, elems: 3\n",
    "d = np.array([[1,2,3]])\n",
    "\n",
    "# rank: 0, elems: 1\n",
    "e = np.array(1)\n",
    "\n",
    "# rank: 1, elems: 1\n",
    "f = np.array([1])\n",
    "\n",
    "# rank: 3, elems: 8\n",
    "g = np.array([[[1,2], [3,4]], [[5,6], [7,8]]])\n",
    "\n",
    "# rank: 4, elems: 12\n",
    "h = np.array([[[[1,2]], [[3,4]], [[5,6]]], [[[7,8]], [[9,10]], [[11,12]]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Slices and reductions\n",
    "Use slicing or `tf.reduce_mean`, `tf.reduce_sum`, and `tf.reduce_any` on the tensors defined below to print:\n",
    " * The (1-2-3)-st element of `a`\n",
    " * The first column of `b`\n",
    " * The shape-(2, 3, 2) tensor obtained by selecting the second and third elements of the third axis of `a`\n",
    " * The sum of all values in `b`\n",
    " * The 2-vector containing means of each row of `b` \n",
    " * The (1, 3) tensor containing, for each column in `c`, whether that column contains any `True` values\n",
    " \n",
    "Each statement should take the form \n",
    "```\n",
    "print(sess.run(something[...]))\n",
    "```\n",
    "or \n",
    "```\n",
    "print(sess.run(tf.reduce_something(...)))\n",
    "```\n",
    "Follow each with a comment stating the shape of the output.\n",
    "For a rank-2 tensor, the first index specifies row and the second specifies column.\n",
    "Make sure to pay attention to the `axis` and `keepdims` arguments of the `reduce` functions.\n",
    " \n",
    " \n",
    "For this problem, I'll set up the default session and name scope, but for all future problems you'll need to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(np.ones((2, 3, 4))) # Tensor of ones with shape (2, 3, 4)\n",
    "b = tf.constant([[1., 2.], \n",
    "                 [3., 4.]]) # Tensor of the matrix [1 2; 3 4] with shape (2, 2)\n",
    "c = tf.constant([[True, True, False],\n",
    "                 [False, True, False]]) # Binary tensor with shape (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1. 3.]\n",
      "[[[1. 1.]\n",
      "  [1. 1.]\n",
      "  [1. 1.]]\n",
      "\n",
      " [[1. 1.]\n",
      "  [1. 1.]\n",
      "  [1. 1.]]]\n",
      "10.0\n",
      "[1.5 3.5]\n",
      "[[ True  True False]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.name_scope('slices_and_reductions'):\n",
    "        # shape of output: ()\n",
    "        print(sess.run(a[1,2,3]))\n",
    "        \n",
    "        # shape of output: (2)\n",
    "        print(sess.run(b[:,0]))\n",
    "        \n",
    "        # shape of output: (2,3,2)\n",
    "        print(sess.run(a[:,:,1:3]))\n",
    "        \n",
    "        # shape of output: ()\n",
    "        print(sess.run(tf.reduce_sum(b)))\n",
    "        \n",
    "        # shape of output: (2)\n",
    "        print(sess.run(tf.reduce_mean(b, 1)))\n",
    "        \n",
    "        # shape of output: (1,3)\n",
    "        print(sess.run(tf.reduce_any(c, 0, keepdims=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Transposition and reshaping\n",
    "Use `tf.transpose` to print:\n",
    " * `b` with its rows and columns swapped\n",
    " * `a` with its second and third axes swapped; comment its shape\n",
    " \n",
    "Use `tf.reshape` to print:\n",
    " * The values of `b` in a tensor with shape (1, 4)\n",
    " * The values of `b` in a tensor with shape (4, 1)\n",
    " \n",
    "Do this all inside the name scope \"transposition_and_reshaping\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 3.]\n",
      " [2. 4.]]\n",
      "[[[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n",
      "[[1. 2. 3. 4.]]\n",
      "[[1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.name_scope('transposition_and_reshaping'):\n",
    "        print(sess.run(tf.transpose(b)))\n",
    "        \n",
    "        # shape of output: (2,4,3)\n",
    "        print(sess.run(tf.transpose(a, perm=[0, 2, 1])))\n",
    "\n",
    "        print(sess.run(tf.reshape(b, [1, 4])))\n",
    "        \n",
    "        print(sess.run(tf.reshape(b, [4, 1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Computing with Operations and Graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: The dot product (as a sum of scalar products)\n",
    "Write a function `dot_sum()` that takes in two rank-1 tensors `a` and `b` of equal shape and returns a tensor that holds their dot product, $$\\text{result} = a \\cdot b = \\sum_{i = 1}^{\\dim{a}} a_i \\cdot b_i $$\n",
    "\n",
    "The computation should first multiply the elements in $a$ and $b$ into a vector $a \\odot b$ (the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) of $a$ and $b$), then sum across the vector to produce a scalar. \n",
    "Your implementation should be _vectorized_: it should not explicitly use the shape of an input tensor or do any looping.\n",
    "The tensor output by your function must be rank-0.\n",
    "\n",
    "The entire computation should use the name scope \"dot_sum\" and the tensor you return should have the name \"result\".\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * `tf.multiply` (or equivalently, the binary operation *)\n",
    " * `tf.reduce_sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_sum(a, b):\n",
    "    '''\n",
    "    Given rank-1 tensors a and b with equal shapes, return the dot product \n",
    "    of a and b as a rank-0 tensor computed via Hadamard product.\n",
    "    '''\n",
    "    \n",
    "    hadamard = tf.multiply(a, b, name='hadamard')\n",
    "    result = tf.reduce_sum(hadamard, name='result')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: The dot product (as matrix multiplication)\n",
    "Write a function `dot_multiply()` that takes in two rank-1 tensors `a` and `b` of equal shape and returns a tensor that holds their dot product, $$\\text{result} = a \\cdot b = a^T b $$\n",
    "\n",
    "The computation should use `tf.matmul` to perform the multiplication, which expects that your input tensors have rank of at least two (they should be matrices, not vectors).\n",
    "Since your input vectors are rank-1, this means you'll need to use `tf.expand_dims` with `axis=-1` to add a \"dummy dimension\".\n",
    "This is a subtle but important point: your vectors start with shape [n], but matrix multiplication is only defined for matrices with shapes [1, n] and [n, 1].\n",
    "Depending on how you do it, you will probably get a rank-2 tensor with a shape like [1, 1].\n",
    "You must return a rank-0 tensor, so use `tf.squeeze` to eliminate dummy dimensions.\n",
    "\n",
    "The entire computation should use the name scope \"dot_multiply\" and the tensor you return should have |the name \"result\".\n",
    "This will not collide with the previous \"result\" tensor because of name scoping.\n",
    "(If it did, it would be renamed to \"result_0\" in the graph)\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * `tf.matmul`\n",
    " * `tf.transpose`\n",
    " * `tf.expand_dims`\n",
    " * `tf.squeeze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_multiply(a, b):\n",
    "    '''\n",
    "    Given rank-1 tensors a and b with equal shapes, return the dot product \n",
    "    of a and b as a rank-0 tensor computed via matrix multiplication.\n",
    "    '''\n",
    "    with tf.name_scope('transposition_and_reshaping'):\n",
    "        a = tf.expand_dims(a, -1, name='a')\n",
    "        b = tf.expand_dims(b, -1, name='b')\n",
    "        result_bad = tf.matmul(tf.transpose(a), b, name='matmul')\n",
    "        result = tf.squeeze(result_bad, name='result')\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: A single ReLU unit\n",
    "The \"default\" activation function for modern neural networks is the [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) (or \"ReLU\"):\n",
    "$$ \\text{relu}(x) = max(0, x). $$\n",
    "\n",
    "In a neural network using ReLU activation, a single unit with $n$ inputs has parameters $w$ (an $n$-vector of weights) and $b$ (a scalar).\n",
    "It computes the function\n",
    "$$ f(x; w, b) = \\text{relu}(w \\cdot x + b). $$\n",
    "\n",
    "Using either `dot_sum` or `dot_multiply`, add these tensors and operations to the default graph:\n",
    "$$\n",
    "\\begin{align}\n",
    "&x: \\space \\text{placeholder} \\\\\n",
    "&w = \\begin{bmatrix}2 & 0.5 & -1\\end{bmatrix} \\\\\n",
    "&b = 0.3 \\\\\n",
    "&\\text{state} = w \\cdot x + b \\\\\n",
    "&\\text{activation} = \\max(\\text{state}, 0)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "`x` should have shape [3] and dtype `tf.float32`, and all tensors should be named, under the name scope \"ReLU\".\n",
    "This includes the tensors created through your dot product function, but do not change your implementation to add to the name!\n",
    "\n",
    "Then, use a default `tf.Session` to evaluate and print `activation` for:\n",
    " * $x = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} -1 & 2 & 0 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} 0 & 0 & 0 \\end{bmatrix}$\n",
    "\n",
    "You should only call your dot product function once.\n",
    "Note that calling the function _adds operations to the graph_: doing it multiple times will create many new operations and tensors.\n",
    "Instead, you want to create them once and evaluate the same `activation` tensor multiple times.\n",
    "Recall that a tensor is just a placeholder and may take different values in different runs.\n",
    "\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * tf.constant\n",
    " * tf.placeholder\n",
    " * tf.add\n",
    " * tf.maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('ReLU'):\n",
    "    x = tf.placeholder(tf.float32, (3), name = 'x')\n",
    "    w = tf.constant([2, 0.5, -1])\n",
    "    b = tf.constant(0.3)\n",
    "    state = tf.add(dot_multiply(w, x), b)\n",
    "    activation = tf.maximum(0., state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8\n",
      "0.0\n",
      "2.3\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(sess.run(activation, feed_dict={x: [1, 1, 1]}))\n",
    "    print(sess.run(activation, feed_dict={x: [-1, 2, 0]}))\n",
    "    print(sess.run(activation, feed_dict={x: [1, 0, 0]}))\n",
    "    print(sess.run(activation, feed_dict={x: [0, 0, 0]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside on activation functions\n",
    "\n",
    "One way to derive feedforward neural networks is to begin by saying \"I'd like to do a simple (linear) transformation on my input features to make them easier to model, then use a simple model (e.g. linear regression) that instead uses the transformed features.\"\n",
    "Doing this means your total model is $y = ABx + b$ where $B$ is the matrix multiplying an input point $x$ into a new representation and $A$ is the matrix parameterizing the linear regression.\n",
    "\n",
    "But, $AB$ is just another matrix, and so by adding a representation you have not made your model more powerful; instead if you'd \"twisted\" the input space after appyling B, the overall map would be nonlinear and the composite model would have greater representation power.\n",
    "Activation functions perform this \"twisting\".\n",
    "Deep neural networks come from the observation that it'd be easier to get a good representation (top layer) if it was based on a lower-level representation (early layers).\n",
    "\n",
    "Here's a great article explaining the geometric interpretation of activation functions: https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/.\n",
    "The general idea is that neural networks can learn parameters that use the \"twists\" such that the entire network deforms space so that the manifold defined by your input data is simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: Graph optimizations\n",
    "You don't have to write code for this section, just look over the cell below, execute it and look at the output.\n",
    "\n",
    "`tf.Print` is an operation that takes a tensor and makes a copy of it that prints itself to the terminal when evaluated.\n",
    "Unfortunately it prints to the terminal running the jupyter notebook and not the notebook itself, so you'll have to look at the terminal to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ba1f04391d2b>:6: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
      "Instructions for updating:\n",
      "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
      "```python\n",
      "    sess = tf.Session()\n",
      "    with sess.as_default():\n",
      "        tensor = tf.range(10)\n",
      "        print_op = tf.print(tensor)\n",
      "        with tf.control_dependencies([print_op]):\n",
      "          out = tf.add(tensor, tensor)\n",
      "        sess.run(out)\n",
      "    ```\n",
      "Additionally, to use tf.print in python 2.7, users must make sure to import\n",
      "the following:\n",
      "\n",
      "  `from __future__ import print_function`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('subgraph_execution'):\n",
    "    a = tf.constant(1, name='a')\n",
    "    b = tf.constant(2, name='b')\n",
    "\n",
    "    # 'print_tensor' is a tensor that's a copy of 'a' except it prints '1' when evaluated \n",
    "    print_tensor = tf.Print(a, [a], name='print_tensor')\n",
    "    \n",
    "    c = tf.add(print_tensor, b, name='c')\n",
    "    d = tf.multiply(print_tensor, c, name='d')\n",
    "    e = tf.add(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(d)\n",
    "    sess.run(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that computing `d` uses the value of `print_tensor` twice, but when we evaluate `e`, the value is printed only once.\n",
    "This indicates that TensorFlow is caching the value of `d` instead of computing it multiple times.\n",
    "When computing `e`, the value is not printed at all because it does not depend on the value of `print_tensor`.\n",
    "These are the simplest two graph optimizations that TensorFlow can make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5: Runs vs sessions\n",
    "Again, just run the code and look at the output.\n",
    "\n",
    "A `tf.Session` object contains the _context_ of every run that happens within it, but that doesn't mean things stay the same within the session.\n",
    "Within a single _run_, tensors always have the same value. \n",
    "The below code generates a random number tensor, then makes a second tensor by adding 1 to the first one, and prints both twice within the same session in separate runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79343283, 1.7934328]\n",
      "[0.8040774, 1.8040774]\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('random_experiment'):\n",
    "    random = tf.random_uniform(())\n",
    "    plus_one = random + 1\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([random, plus_one]))\n",
    "    print(sess.run([random, plus_one]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within a single run, the values of `random` and `plus_one` are consistent, so `random`'s value is fixed during the run.\n",
    "In the second run of the session, `random` and `plus_one` take on different values than they did in the first run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing a function with gradient descent\n",
    "Minimize the scalar function $f(x) = (x-1)(2x-2)(x-3)(x-4)$, plotted below, using gradient descent.\n",
    "It has a local minimum near $x = 1$ and a global minimum near $x = 3.5$.\n",
    "\n",
    "![f(x)](./images/plot_f.png)\n",
    "\n",
    "The steps to build the graph are:\n",
    " 1. Use `tf.get_variable` to get a variable named `x` that uses a `tf.random_uniform_initializer` on the range [-1, 5] \n",
    " 2. Make a tensor `y` that represents f(x) given a value of `x`\n",
    " 3. Make a `tf.train.GradientDescentOptimizer` named \"optimizer\" with a learning rate of 0.01\n",
    " 4. Make the operation that performs gradient updates by using the `minimize` function of the optimizer on `y`. Name it `gradient_step`.\n",
    " \n",
    "Only build the graph once!\n",
    "The whole subgraph for this problem should go under a name scope of \"minimize_f\", and operations to compute `y` should have an additional name scope of \"compute_f\".\n",
    "\n",
    "Then, the steps to minimize the function once are:\n",
    " 1. Run `tf.global_variables_initializer()` to initialize `x`\n",
    " 2. Print the initial values of `x` and `y`\n",
    " 3. Run `gradient_step` 1000 times\n",
    " 4. Print the final values of x and y\n",
    " \n",
    "Minimize the function a few times. If you did it right, you'll find that in each run the optimizer finds one of two minima. Running minimization a few times, you should see it find both eventually. What determines which minimum is found? Answer in the markdown box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Your graph code here\n",
    "\n",
    "with tf.name_scope('minimize_f'):\n",
    "    \n",
    "    x = tf.get_variable('x', shape=(), dtype=tf.float32, \n",
    "                        initializer=tf.random_uniform_initializer(minval=-1, maxval=5)) \n",
    "    \n",
    "    with tf.name_scope('compute_f'):\n",
    "        y = (x - 1) * ((2 * x) - 2) * (x - 3) * (x - 4)\n",
    "    \n",
    "    # Create optimization operator\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01, name='optimizer')\n",
    "    gradient_step = optimizer.minimize(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value of x: 2.6389794\n",
      "Initial value of f(x): 2.6398158\n",
      "Final value of x: 3.59307\n",
      "Final value of f(x): -3.245519\n"
     ]
    }
   ],
   "source": [
    "# Your session code here\n",
    "\n",
    "# Run training\n",
    "with tf.Session() as sess: # Create a session   \n",
    "    sess.run(tf.global_variables_initializer()) # Initialize all variables\n",
    "    \n",
    "    print('Initial value of x:', sess.run(x))\n",
    "    print('Initial value of f(x):', sess.run(y))\n",
    "    \n",
    "    for step in range(1000):\n",
    "        sess.run(gradient_step)\n",
    "        \n",
    "    print('Final value of x:', sess.run(x))\n",
    "    print('Final value of f(x):', sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two minimums which can be found: 1 and 3.59. The variable $x$ where the gradient descent algorithm starts is initialized uniformly on $[-1,5]$. If $x$ is initialized (roughly) into the interval $[-1,2]$, then the gradient descent optimizer finds the local minimum $x=1$, and otherwise its finds the global minimum of $x=3.59$. This is because we travel in the direction of the negative gradient, and depending on where we start the algorithm, we can \"descend\" into the two different minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
